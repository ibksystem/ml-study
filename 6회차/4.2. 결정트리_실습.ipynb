{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. 분류(Classification)의 개요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지도학습은 레이블(Label), 즉 명시적인 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식\n",
    "\n",
    "지도학습의 대표적인 유형인 분류는 \n",
    "학습 데이터로 주어진 데이터의 피처와 레이블값(결정 값, 클래스 값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고, 이렇게 생성된 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측하는 것.\n",
    "\n",
    "다양한 머신러닝 알고리즘으로 구현 가능\n",
    "* 베이즈 통계와 생성 모델에 기반한 나이브 베이즈\n",
    "* 로지스틱 회귀\n",
    "* 결정 트리\n",
    "* 벡터 머신\n",
    "* 최소 근접 (Nearest Neighbor) 알고리즘\n",
    "* 신경망 (Newral Network)\n",
    "* 앙상블 (Ensemble)\n",
    "\n",
    "이 장에서는 분류에서 가장 각광받는 앙상블 방법을 집중적으로 다룹니다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 앙상블 방법 (Ensemble Method)\n",
    "\n",
    "* 이미지, 영상, 음성, NLP 영역에서 신경망에 기반한 딥러닝이 머신러닝계를 선도\n",
    "* 이를 제외한 정형 데이터의 예측 분석 영역에서 앙상블이 높은 예측 성능으로 애용됨\n",
    "* 일반적으로 Bagging, Boosting 방식으로 나뉨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FcDRbo3%2FbtqEPer6ve4%2FhwE68E26Cje11qasksRRVk%2Fimg.png\" width=\"600\" />\n",
    "\n",
    "<a href=\"https://brunch.co.kr/@hvnpoet/95\" target=\"_blank\">https://brunch.co.kr/@hvnpoet/95</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결정 트리\n",
    "\n",
    "* 앙상블의 기본 알고리즘으로 사용\n",
    "* 장점: 매우 쉽고 유연하게 적용 가능한 알고리즘. 데이터의 스케일링, 정규화 등 사전 가공의 영향이 매우 적음\n",
    "* 단점: 예측 성능을 향상시키기 위해 복잡한 규칙 구조를 가져야 하며, 이로 인한 과적함(overfitting)이 발생해 예측 성능이 저하될 수 있음\n",
    "\n",
    "단점이 앙상블 기법에서는 장점으로 작용. \n",
    "앙상블은 매우 많은 여러 개의 약한 학습기를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성능을 향상시킴. 결정 트리가 좋은 약한 학습기가 되어줌\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/2400/1*bYGSIgMlmVdedFJaE6PuBg.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. 결정 트리 (Decision Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것\n",
    "* if/else 기반으로 나타냄\n",
    "* 쉽게 생각하면 스무고개 게임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tensorflowkorea.files.wordpress.com/2017/06/2-22.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*STEB0KajdBmOebaEqwhz1w.png\" width=\"600\" />\n",
    "\n",
    "* 루트 노드\n",
    "* 규칙 노드 (Decision Node)\n",
    "* 리프 노드 (Leaf Node)\n",
    "* 새로운 규칙 조건마다 서브 트리 생성 (Sub Tree)\n",
    "\n",
    "<br/>\n",
    "\n",
    "* 많은 규칙 -> 과적합 (Overfitting)\n",
    "* 트리의 깊이(depth) 가 깊어질수록 결정 트리의 예측 성능 저하\n",
    "* 가능한 한 적은 결정 노드로 높은 예측 정확도를 가지도록 -> 최대한 균일한 데이터 세트를 구성할 수 있게 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 의사결정트리 적용 예시 (멀티캠퍼스 강의 참고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FPqtAd%2FbtqENZXfyzH%2FZyDKQfGGuKJGwG6qiJKkjk%2Fimg.jpg\" />\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbyiZas%2FbtqEN9SUgxy%2FRTUqdykLV5AqkqKRz7kng0%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbMiWvd%2FbtqENYjL2QL%2FJTK6k2ZYd2SCRiRRR354L1%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbozehL%2FbtqEOaEhU0A%2FK0ugLdPK9MGkXHCMDWnHqk%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2F30ujS%2FbtqEPlxQjVq%2F1F2K9YF9xBhVkj0IOPfBm1%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FCwi2X%2FbtqEN80IL8T%2FKsVskTk8wAuXYaFqzSzwNk%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FcdSf8z%2FbtqENZJHXO9%2FbuUZLqLyY6KcUb3kz07Pv1%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbueDjL%2FbtqENY47Ly7%2FDe9fGp0dAt0JvoRP9zmrE0%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fc7KilD%2FbtqEOZvgRTD%2FQQ3Zwd0MMkEcHtBU4vE6e0%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbrMNIa%2FbtqEP4a5075%2FKt7YkwETRNYZHJ16Rf9oMk%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fd5h2AD%2FbtqEOOncnRF%2FqDxKWxbDhk6FdyMIAn7HZk%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fbs68xo%2FbtqEOEE3p9B%2F6lwoo4HG6Iakgli2wE3EL1%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fdbj3u0%2FbtqEP3cbr80%2Fzkt5jNMZ4bpM0b8WG4oFpK%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FNXUU1%2FbtqEOZvgRVV%2FrlpRuxqWkqFayYolU2c331%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbL4OwR%2FbtqEOOU3kQj%2FeRXo26eKlki4AZkpk7A9y1%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fbhg3w2%2FbtqEOFqwBYo%2F28MXKhgFv4K0SWBJEUI1VK%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbQVBf4%2FbtqEP3pH9Ks%2FCbHUsnwkg8m0yWHJHLhvc1%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbKf7hC%2FbtqEOO1LTUG%2Fq31mrCP3mfPRmF0VB0BCf0%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fbovgwh%2FbtqENZXfyB4%2FxPiyINSJJ7sVRnxsfdJ0kK%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FnNnSF%2FbtqEPUNiBJb%2FNTHhR44E4dKF5ZfPUWvNXk%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FAInpg%2FbtqENZXfyD2%2F3Pi9H1hot0glu3vhAJSKMk%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2Fdjl81f%2FbtqEOZovro1%2FsXvpTkQguuSLNbk4Bo2xj1%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbNGuOt%2FbtqEPU0P2Y7%2FSsfErDzrpJIdTLcNoHABkK%2Fimg.jpg\">\n",
    "\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2F402LY%2FbtqEPeTeXiH%2F8Fc4UScbZF79HVeh0RIdN0%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FmNifo%2FbtqEPvmGt0v%2FujF3rrPGwI7DqPZyj6s3j0%2Fimg.jpg\">\n",
    "\n",
    "<br/>\n",
    "<img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbDWJLs%2FbtqEPU0P21K%2FOD1QuvsxRCsCpKk2dtbC11%2Fimg.jpg\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정보의 균일도 측정 방법\n",
    "\n",
    "### 정보 이득\n",
    "\n",
    "* 엔트로피로 측정\n",
    "* 서로 다른 값이 섞여 있으면 엔트로피 높고, 같은 값 섞여 있으면 엔트로피 낮음\n",
    "* 정보 이득 지수 = 1 - 엔트로피 지수\n",
    "\n",
    "### 지니 계수 \n",
    "\n",
    "* 원래 경제학에서 불평등 지수를 나타낼 때 사용하는 계수\n",
    "* 0 평등 \n",
    "* 1 불평등\n",
    "* 사이킷런 DecisionTreeClassifier 기본적으로 지니 계수를 이용해 데이터 세트 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결정 트리 모델의 특징\n",
    "\n",
    "## 장점\n",
    "\n",
    "* 정보의 '균일도'라는 룰을 기반\n",
    "* 알고리즘이 쉽고 직관적\n",
    "* 특별한 경우 제외하고 각 피처의 스케일링, 정규화 등의 전처리 작업이 필요 없음\n",
    "\n",
    "## 단점\n",
    "\n",
    "* 과적합으로 정확도가 떨어짐\n",
    "* 트리의 크기를 사전에 제한하는 튜닝 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결정 트리 파라미터\n",
    "\n",
    "## DecisionTreeClassifier\n",
    "### Parameter\n",
    "\n",
    "#### min_samples_split : int or float, default=2\n",
    "\n",
    "* 노드를 분할하기 위한 최소한의 샘플 데이터 수로 과적합을 제어하는 데 사용\n",
    "* 디폴트 2, 작게 설정할수록 분할되는 노드 많아져서 과적합 가능성 증가\n",
    "* 1로 설정하면 분할되는 노드 많아져서 과적합 가능성 증가\n",
    "\n",
    "#### min_samples_leaf : int or float, default=1\n",
    "\n",
    "* 말단 노드(Leaf)가 되기 위한 최소한의 샘플 데이터 수\n",
    "* 과적합 제어 용도. 비대칭적 (imbalanced) 데이터의 경우 특정 클레스의 데이터가 극도로 작을 수 있으므로 그럴 경우 작게 설정 필요\n",
    "\n",
    "#### max_features\n",
    "\n",
    "* 최적의 분할을 위해 고려할 최대 피처 개수\n",
    "* 디폴트: None. 데이터 세트의 모든 피처를 사용해 분할 수행\n",
    "* int 형으로 지정하면 대상 피처의 개수, float 형으로 지정하면 전체 피처 중 대상 피처의 퍼센트\n",
    "* sqrt : 전체 피처 중 sqrt(전체 피처 개수) 만큼 선정\n",
    "* auto 로 지정 : sqrt 와 동일\n",
    "* log 는 전체 피처 중 log2(전체 피처개수) 선정\n",
    "* None 은 전체 피처 선정\n",
    "\n",
    "#### max_depth\n",
    "\n",
    "* 트리의 최대 깊이 규정\n",
    "* 디폴트: None. None 으로 설정시 완벽하게 클래스 결정 값이 될때까지 깊이를 계속 키우며 분할하거나, 노드가 가지는 데이터 개수가 min_samples_split 보다 작아질 떄까지 계속 깊이 증가시킴\n",
    "* 깊이가 깊어지면 min_samples_split 설정대로 최대 분할하여 과적합할 수 있으므로 적절한 값으로 제어 필요\n",
    "\n",
    "#### max_leaf_nodes\n",
    "\n",
    "* 말단 노드(Leaf)의 최대 개수\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결정 트리 모델의 시각화\n",
    "\n",
    "* Graphviz 패키지 사용하여 트리 시각화\n",
    "* <a href=\"https://www.graphviz.org\" target=\"_blank\">https://www.graphviz.org</a>\n",
    "* export_graphviz() API 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설치 \n",
    "\n",
    "* conda install graphviz\n",
    "* conda install python-graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://jidan.sinkpoint.com/images/tf_logisticregression_deco.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://www.r2d3.us/?from=@\" target=\"_blank\">http://www.r2d3.us/?from=@</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#DecisionTree Classifier 생성\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "\n",
    "# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)\n",
    "\n",
    "#DecisionTrxeeClassifier 학습\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# 위에서 생성된 tree.dot 파일을 Graphviz 가 읽어서 주피터 노트북상에서 시각화\n",
    "export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    \n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* petal: 꽃잎 / sepal: 꽃받침\n",
    "* petal length(cm) <= 2.45 와 같이 조건이 있는 것은 자식 노드를 만들기 위한 규칙 조건. 조건식이 없는 것은 리프 노드\n",
    "* gini : value = [] 로 주어진 데이터 분포에서의 지니 계수\n",
    "* samples: 현 규칙에 해당하는 데이터 건수\n",
    "* value = [] 클래스 값 기반의 데이터 건수. 붓꽃 데이터 세트 0: Setosa, 1: Versicolor, 2: Virginica 품종\n",
    "ex) value = [41,40,39] : Setosa 41개, Versicolor 40개, Virginica 39개로 데이터 구성되어 있다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 색깔이 짙어질수록 지니 계수가 낮고, 해당 레이블에 속하는 샘플 데이터가 많다는 의미"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*7GDzi5knHIqxdU3xys6dvA.png\" />\n",
    "<br/>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*wSWO73IhTa8EEqPqgER8Qw.png\" />\n",
    "<br/>\n",
    "<img src=\"https://miro.medium.com/max/1400/1*cJrVFpo38_DwxACM7IVXJg.png\" />\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처: <a href=\"https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952\" target=\"_blank\">https://towardsdatascience.com/understanding-decision-trees-for-classification-python-9663d683c952</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# max_depth=3 으로 설정\n",
    "dt_clf = DecisionTreeClassifier(random_state=156, max_depth=3)\n",
    "\n",
    "# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)\n",
    "\n",
    "#DecisionTrxeeClassifier 학습\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# 위에서 생성된 tree.dot 파일을 Graphviz 가 읽어서 주피터 노트북상에서 시각화\n",
    "export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    \n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  min_samples_split=4 설정, 자식 노드로 분기할 수 있는 최소한의 샘플 수\n",
    "dt_clf = DecisionTreeClassifier(random_state=156, min_samples_split=4)\n",
    "\n",
    "# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)\n",
    "\n",
    "#DecisionTrxeeClassifier 학습\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# 위에서 생성된 tree.dot 파일을 Graphviz 가 읽어서 주피터 노트북상에서 시각화\n",
    "export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    \n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자식 노드로 분기하려면 최소한 sample 이 4개 있어야 하는데 3개뿐이므로 분기 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  min_samples_leaf=4 설정\n",
    "dt_clf = DecisionTreeClassifier(random_state=156, min_samples_leaf=4)\n",
    "\n",
    "# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)\n",
    "\n",
    "#DecisionTrxeeClassifier 학습\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# 위에서 생성된 tree.dot 파일을 Graphviz 가 읽어서 주피터 노트북상에서 시각화\n",
    "export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    \n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 이 4 이상인 노드는 리프 클래스 노드가 될 수 있으므로 규칙이 samples = 4 인 노드를 만들 수 있는 상황을 반영하여 변경됩니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  max_leaf_nodes=2 설정\n",
    "dt_clf = DecisionTreeClassifier(random_state=156, max_leaf_nodes=2)\n",
    "\n",
    "# 붓꽃 데이터를 로딩하고, 학습과 테스트 데이터 세트로 분리\n",
    "iris_data = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2, random_state=11)\n",
    "\n",
    "#DecisionTrxeeClassifier 학습\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "# 위에서 생성된 tree.dot 파일을 Graphviz 가 읽어서 주피터 노트북상에서 시각화\n",
    "export_graphviz(dt_clf, out_file=\"tree.dot\", class_names=iris_data.target_names, feature_names=iris_data.feature_names, impurity=True, filled=True)\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "    \n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 결정 트리는 균일도에 기반해 어떤 속성을 규칙 조건으로 선택하느냐가 중요한 요건\n",
    "* 중요한 몇 개의 피처가 명확한 규칙 트리를 만드는 데 크게 기여\n",
    "* 피처의 중요도 DecisionTreeClassifier 객체의 feature_importances_ 속성으로 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#feature importance \n",
    "print('Feature importance:\\n{0}'.format(np.round(dt_clf.feature_importances_, 3)))\n",
    "\n",
    "#feature - importance mapping\n",
    "for name, value in zip(iris_data.feature_names, dt_clf.feature_importances_):\n",
    "    print('{0} : {1:.3f}'.format(name, value))\n",
    "    \n",
    "#feature importance - visualize by column\n",
    "sns.barplot(x=dt_clf.feature_importances_, y=iris_data.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>결정 트리 과적합(Overfitting)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류를 위한 테스트용 데이터를 쉽게 만들 수 있도록 제공하는 함수 make_classification()\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.title(\"3 Class values with 2 Features Sample data creation\")\n",
    "\n",
    "# 2차원 시각화를 위해서 피처는 2개, 클래스는 3가지 유형의 분류 샘플 데이터 생성\n",
    "X_features, y_labels = make_classification(n_features=2, n_redundant=0, n_informative=2, n_classes=3, n_clusters_per_class=1, random_state=0)\n",
    "\n",
    "# 그래프 형태로 2개의 피처로 2차원 좌표 시각화, 각 클래스 값은 다른 색깔로 표시됨\n",
    "plt.scatter(X_features[:, 0], X_features[:, 1], marker='o', c=y_labels, s=25, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Classifier의 Decision Boundary를 시각화 하는 함수\n",
    "def visualize_boundary(model, X, y):\n",
    "    fig,ax = plt.subplots()\n",
    "    \n",
    "    # 학습 데이타 scatter plot으로 나타내기\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=25, cmap='rainbow', edgecolor='k',\n",
    "               clim=(y.min(), y.max()), zorder=3)\n",
    "    ax.axis('tight')\n",
    "    ax.axis('off')\n",
    "    xlim_start , xlim_end = ax.get_xlim()\n",
    "    ylim_start , ylim_end = ax.get_ylim()\n",
    "    \n",
    "    # 호출 파라미터로 들어온 training 데이타로 model 학습 . \n",
    "    model.fit(X, y)\n",
    "    # meshgrid 형태인 모든 좌표값으로 예측 수행. \n",
    "    xx, yy = np.meshgrid(np.linspace(xlim_start,xlim_end, num=200),np.linspace(ylim_start,ylim_end, num=200))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "    \n",
    "    # contourf() 를 이용하여 class boundary 를 visualization 수행. \n",
    "    n_classes = len(np.unique(y))\n",
    "    contours = ax.contourf(xx, yy, Z, alpha=0.3,\n",
    "                           levels=np.arange(n_classes + 1) - 0.5,\n",
    "                           cmap='rainbow', clim=(y.min(), y.max()),\n",
    "                           zorder=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 특정한 트리 생성 제약 없는 결정 트리의 학습과 결정 경계 시각화\n",
    "dt_clf = DecisionTreeClassifier().fit(X_features, y_labels)\n",
    "visualize_boundary(dt_clf, X_features, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 일부 이상치(Outlier) 데이터까지 분류하기 위해 분할이 자주 일어나서 결정 기준 경계가 매우 많아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_samples_leaf = 6으로 트리 생성 조건을 제약한 결정 경계 시각화\n",
    "dt_clf = DecisionTreeClassifier(min_samples_leaf=6).fit(X_features, y_labels)\n",
    "visualize_boundary(dt_clf, X_features, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 이상치에 크게 반응하지 않으면서 일반화된 분류 규칙에 따라 분류됨\n",
    "* 두 번째 모델이 더 뛰어날 가능성이 높음\n",
    "* 테스트 데이터 세트는 학습 데이터 세트와는 다른 데이터 세트인데, 학습 데이터에만 지나치게 최적화된 분류 기준은 오히려 테스트 데이터 세트에서 정확도를 떨어뜨릴 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결정 트리 실습 - 사용자 행동 인식 데이터 세트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 30명에게 스마트폰 센서를 장착한 뒤 사람의 동작과 관련된 여러 가지 피처를 수집한 데이터\n",
    "* <a href=\"https://archive.ics.uci.edu/ml/index.php\" target=\"_blank\">https://archive.ics.uci.edu/ml/index.php</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# features.txt 파일에는 피처 이름 index 와 피처명이 공백으로 분리되어있음. 이를 DataFrame 으로 로드\n",
    "feature_name_df = pd.read_csv('./human_activity/features.txt', sep='\\s+', header=None, names=['column_index', 'column_name'])\n",
    "\n",
    "# 피처명 index 를 제거하고, 피처명만 리스트 객체로 생성한 뒤 샘플로 10개만 추출\n",
    "feature_name = feature_name_df.iloc[:, 1].values.tolist()\n",
    "print('전체 피처명에서 10개만 추출:', feature_name[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 인체의 움직임과 관련된 속성의 평균/표준편차가 X, Y, X 축 값으로 되어 있음\n",
    "\n",
    "<img src=\"https://static1.squarespace.com/static/597d2753be6594cef6a34840/597d2c4715d5dbc171e4723f/597ded44e45a7c9f617e440b/1504188481214/micromachines-06-01100-g002.png?format=2500w\" />\n",
    "\n",
    "<a href=\"https://www.mikechatzidakis.com/home/2017/7/30/human-activity-recognition-with-smartphones\" target=\"_blank\">https://www.mikechatzidakis.com/home/2017/7/30/human-activity-recognition-with-smartphones</a>\n",
    "    \n",
    "<a href=\"https://papago.naver.net/website?locale=ko&source=auto&target=ko&url=https%3A%2F%2Fwww.mikechatzidakis.com%2Fhome%2F2017%2F7%2F30%2Fhuman-activity-recognition-with-smartphones\" target=\"_blank\"> 위 글의 번역글(파파고) </a>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 스마트폰 내의 gyroscope 로 측정\n",
    "* 6가지 활동(계단 오르기/ 내리기, 걷기, 앉기, 달리기, 누워 있기 및 서 있기)을 수행하는 30명의 개인이 기록한 데이터(299개 관측치 10개, 562개 변수) 포함\n",
    "* 이 데이터 세트 기록 목적: 의사들이 집에 있는 노인 환자들의 신체 활동을 관찰하기 위해 스마트 폰을 사용하기를 원함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 수정 버전 01: 날짜 2019.10.27일\n",
    "\n",
    "**원본 데이터에 중복된 Feature 명으로 인하여 신규 버전의 Pandas에서 Duplicate name 에러를 발생.**  \n",
    "**중복 feature명에 대해서 원본 feature 명에 '_1(또는2)'를 추가로 부여하는 함수인 get_new_feature_name_df() 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name'] = new_feature_name_df[['column_name', 'dup_cnt']].apply(lambda x : x[0]+'_'+str(x[1]) \n",
    "                                                                                           if x[1] >0 else x[0] ,  axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'], axis=1)\n",
    "    return new_feature_name_df\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "new_feature_name_df[new_feature_name_df['dup_cnt'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 get_human_dataset() 함수는 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df() 함수를 반영하여 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_human_dataset( ):\n",
    "    \n",
    "    # 각 데이터 파일들은 공백으로 분리되어 있으므로 read_csv에서 공백 문자를 sep으로 할당.\n",
    "    feature_name_df = pd.read_csv('./human_activity/features.txt',sep='\\s+',\n",
    "                        header=None,names=['column_index','column_name'])\n",
    "    \n",
    "    # 중복된 feature명을 새롭게 수정하는 get_new_feature_name_df()를 이용하여 새로운 feature명 DataFrame생성. \n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    \n",
    "    # DataFrame에 피처명을 컬럼으로 부여하기 위해 리스트 객체로 다시 변환\n",
    "    feature_name = new_feature_name_df.iloc[:, 1].values.tolist()\n",
    "    \n",
    "    # 학습 피처 데이터 셋과 테스트 피처 데이터을 DataFrame으로 로딩. 컬럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('./human_activity/train/X_train.txt',sep='\\s+', names=feature_name )\n",
    "    X_test = pd.read_csv('./human_activity/test/X_test.txt',sep='\\s+', names=feature_name)\n",
    "    \n",
    "    # 학습 레이블과 테스트 레이블 데이터를 DataFrame으로 로딩하고 컬럼명은 action으로 부여\n",
    "    y_train = pd.read_csv('./human_activity/train/y_train.txt',sep='\\s+',header=None,names=['action'])\n",
    "    y_test = pd.read_csv('./human_activity/test/y_test.txt',sep='\\s+',header=None,names=['action'])\n",
    "    \n",
    "    # 로드된 학습/테스트용 DataFrame을 모두 반환 \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('## 학습 피처 데이터셋 info()')\n",
    "print(X_train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 피처가 전부 float 이므로 별도의 카테고리 인코딩 필요 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train['action'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier 를 이용해 동작 예측 분류\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 예제 반복 시 마다 동일한 예측 결과 도출을 위해 random_state 설정\n",
    "dt_clf = DecisionTreeClassifier(random_state=156)\n",
    "dt_clf.fit(X_train , y_train)\n",
    "pred = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test , pred)\n",
    "print('결정 트리 예측 정확도: {0:.4f}'.format(accuracy))\n",
    "\n",
    "# DecisionTreeClassifier의 하이퍼 파라미터 추출\n",
    "print('DecisionTreeClassifier 기본 하이퍼 파라미터:\\n', dt_clf.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Depth 가 예측 정확도에 주는 영향 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# max_depth 를 계속 늘리면서 예측 성능 측정\n",
    "params = {\n",
    "    'max_depth' : [ 6, 8 ,10, 12, 16 ,20, 24]\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )\n",
    "grid_cv.fit(X_train , y_train)\n",
    "print('GridSearchCV 최고 평균 정확도 수치:{0:.4f}'.format(grid_cv.best_score_))\n",
    "print('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 수정 버전 01: 날짜 2019.10.27일\n",
    "사이킷런 버전이 업그레이드 되면서 아래의 GridSearchCV 객체의 cv_results_에서 mean_train_score는 더이상 제공되지 않습니다.\n",
    "기존 코드에서 오류가 발생하시면 아래와 같이 'mean_train_score'를 제거해 주십시요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV객체의 cv_results_ 속성을 DataFrame으로 생성. \n",
    "cv_results_df = pd.DataFrame(grid_cv.cv_results_)\n",
    "\n",
    "# max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출\n",
    "# 사이킷런 버전이 업그레이드 되면서 아래의 GridSearchCV 객체의 cv_results_에서 mean_train_score는 더이상 제공되지 않습니다\n",
    "# cv_results_df[['param_max_depth', 'mean_test_score', 'mean_train_score']]\n",
    "\n",
    "# max_depth 파라미터 값과 그때의 테스트(Evaluation)셋, 학습 데이터 셋의 정확도 수치 추출\n",
    "cv_results_df[['param_max_depth', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* mean_test_score : 5개의 CV 세트에서 검증용 데이터 세트의 정확도 평균 수치\n",
    "* max_depth __를 넘으면 정확도 하락    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 위 예제에서 가장 좋은 성능을 나타내는 max_depth 를 가지고 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [ 6, 8 ,10, 12, 16 ,20, 24]\n",
    "# max_depth 값을 변화 시키면서 그때마다 학습과 테스트 셋에서의 예측 성능 측정\n",
    "for depth in max_depths:\n",
    "    dt_clf = DecisionTreeClassifier(max_depth=depth, random_state=156)\n",
    "    dt_clf.fit(X_train , y_train)\n",
    "    pred = dt_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    print('max_depth = {0} 정확도: {1:.4f}'.format(depth , accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_samples_split 에도 변화를 주어 정확도 성능 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth' : [ 8 , 12, 16 ,20], \n",
    "    'min_samples_split' : [16,24],\n",
    "}\n",
    "\n",
    "grid_cv = GridSearchCV(dt_clf, param_grid=params, scoring='accuracy', cv=5, verbose=1 )\n",
    "grid_cv.fit(X_train , y_train)\n",
    "print('GridSearchCV 최고 평균 정확도 수치: {0:.4f}'.format(grid_cv.best_score_))\n",
    "print('GridSearchCV 최적 하이퍼 파라미터:', grid_cv.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최적 하이퍼 파라미터로 학습이 완료된 Estimator 객체로 테스트 데이터 세트 예측 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_df_clf = grid_cv.best_estimator_\n",
    "\n",
    "pred1 = best_df_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test , pred1)\n",
    "print('결정 트리 예측 정확도:{0:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 각 피처의 중요도를 Features_importances_ 속성을 이용해 알아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ftr_importances_values = best_df_clf.feature_importances_\n",
    "\n",
    "# Top 중요도로 정렬을 쉽게 하고, 시본(Seaborn)의 막대그래프로 쉽게 표현하기 위해 Series변환\n",
    "ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns  )\n",
    "\n",
    "# 중요도값 순으로 Series를 정렬\n",
    "ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature importances Top 20')\n",
    "sns.barplot(x=ftr_top20 , y = ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
